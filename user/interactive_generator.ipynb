{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review response generation (demo)\n",
    "\n",
    "This is a demo script for automatic response generation models trained with Fairseq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# set device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"  # specify which GPU(s) to be used\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from fairseq import bleu, checkpoint_utils, data, options, tasks, utils\n",
    "from fairseq.data import encoders\n",
    "from fairseq.logging.meters import StopwatchMeter\n",
    "\n",
    "\n",
    "# from fairseq.models.rrgen_seq2seq_lstm import rrgen_lstm_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define arguments and load model for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from /srv/scratch2/kew/fairseq_materials/rrgen/de/ft100src_rg/checkpoints/checkpoint_best.pt\n",
      "finished loading model from /srv/scratch2/kew/fairseq_materials/rrgen/de/ft100src_rg/checkpoints/checkpoint_best.pt\n"
     ]
    }
   ],
   "source": [
    "# Parse command-line arguments for generation\n",
    "parser = options.get_generation_parser(default_task='rrgen_translation')\n",
    "\n",
    "# test (small) model\n",
    "# input_args = [\n",
    "#     '/srv/scratch2/kew/fairseq_materials/translation_format/raw',\n",
    "#     '--path=/srv/scratch2/kew/fairseq_materials/translation_format/rrgen_lstm_emb_senti_cate_rate_100k/checkpoints/checkpoint_best.pt',\n",
    "#     '-s=src',\n",
    "#     '-t=tgt',\n",
    "#     '--task=rrgen_translation',\n",
    "#     '--dataset-impl=raw',\n",
    "#     '--nbest=1',\n",
    "#     '--beam=10',\n",
    "# #     '--sampling',\n",
    "# #     '--sampling-topp=0.9',\n",
    "#     '--use-sentiment=senti',\n",
    "#     '--use-category=cate',\n",
    "#     '--use-rating=rate',\n",
    "# ]\n",
    "\n",
    "input_args = [\n",
    "    '/srv/scratch2/kew/fairseq_materials/rrgen/de/data_bin_rg',\n",
    "    '--path=/srv/scratch2/kew/fairseq_materials/rrgen/de/ft100src_rg/checkpoints/checkpoint_best.pt',\n",
    "    '-s=review',\n",
    "    '-t=response_rg',\n",
    "    '--task=rrgen_translation',\n",
    "    '--dataset-impl=mmap',\n",
    "    '--nbest=1',\n",
    "    '--beam=5',\n",
    "#     '--sampling',\n",
    "#     '--sampling-topk=10',\n",
    "#     '--sampling-topp=0.98',\n",
    "    '--use-sentiment=sentiment',\n",
    "    '--use-category=domain',\n",
    "    '--use-rating=rating',\n",
    "]\n",
    "\n",
    "args = options.parse_args_and_arch(parser, input_args=input_args)\n",
    "\n",
    "# Set device\n",
    "# use_cuda = torch.cuda.is_available() and not args.cpu\n",
    "use_cuda = False\n",
    "\n",
    "# Setup task\n",
    "task = tasks.setup_task(args)\n",
    "\n",
    "# Load model\n",
    "print('loading model from {}'.format(args.path))\n",
    "models, _model_args = checkpoint_utils.load_model_ensemble(\n",
    "    [args.path], task=task)\n",
    "model = models[0]\n",
    "\n",
    "print('finished loading model from {}'.format(args.path))\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "# Load alignment dictionary for unknown word replacement\n",
    "# (None if no unknown word replacement, empty if no path to align dictionary)\n",
    "align_dict = utils.load_align_dict(args.replace_unk)\n",
    "\n",
    "# initialise generator model\n",
    "generator = task.build_generator(models, args)\n",
    "\n",
    "# Handle tokenization and BPE\n",
    "tokenizer = encoders.build_tokenizer(args)\n",
    "bpe = encoders.build_bpe(args)\n",
    "\n",
    "def decode_fn(x):\n",
    "    if bpe is not None:\n",
    "        x = bpe.decode(x)\n",
    "    if tokenizer is not None:\n",
    "        x = tokenizer.decode(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(sentence: str,\n",
    "                 ext_senti: Optional[int] = None,\n",
    "                 ext_cate: Optional[str] = None,\n",
    "                 ext_rate: Optional[int] = None,\n",
    "                 target: Optional[str] = None,\n",
    "                ):\n",
    "\n",
    "    # vectorize input sentence \n",
    "    tokens = task.source_dictionary.encode_line(\n",
    "    sentence, add_if_not_exist=False,)\n",
    "\n",
    "    if args.use_sentiment:\n",
    "        try:\n",
    "            ext_senti = task.ext_senti_dict[ext_senti]\n",
    "        except:\n",
    "            try:\n",
    "                ext_senti = task.ext_senti_dict[int(ext_senti)]\n",
    "            except:\n",
    "                r = random.choice(list(task.ext_senti_dict))\n",
    "                print(f'[!] WARNING: Could not find value for sentiment input {ext_senti}. Using sentiment value {r}')               \n",
    "                ext_senti = task.ext_senti_dict[r]\n",
    "    if args.use_category:\n",
    "        try:\n",
    "            ext_cate = task.ext_cate_dict[ext_cate]\n",
    "        except:\n",
    "            try:\n",
    "                ext_cate = task.ext_cate_dict[int(ext_cate)]\n",
    "            except:\n",
    "                r = random.choice(list(task.ext_cate_dict))\n",
    "                print(f'[!] WARNING: Could not find value for category input {ext_cate}. Using category value {r}')               \n",
    "                ext_cate = task.ext_cate_dict[r]\n",
    "    if args.use_rating:\n",
    "        try:\n",
    "            ext_rate = task.ext_rate_dict[ext_rate]\n",
    "        except:\n",
    "            try:\n",
    "                ext_rate = task.ext_rate_dict[int(ext_rate)]\n",
    "            except:\n",
    "                r = random.choice(list(task.ext_rate_dict))\n",
    "                print(f'[!] WARNING: Could not find value for category input {ext_rate}. Using rating value {r}')               \n",
    "                ext_rate = task.ext_rate_dict[r]\n",
    "                \n",
    "    # collate input as batch of size 1\n",
    "    batch = data.rrgen_dataset.collate(\n",
    "    samples=[{\n",
    "        'id': -1,\n",
    "        'source': tokens,\n",
    "        'ext_senti': ext_senti,\n",
    "        'ext_rate': ext_rate,\n",
    "        'ext_cate': ext_cate\n",
    "    }],\n",
    "    pad_idx=task.source_dictionary.pad(),\n",
    "    eos_idx=task.source_dictionary.eos(),\n",
    "    left_pad_source=False,\n",
    "    input_feeding=False)\n",
    "\n",
    "    # [!] ensure correct dtype (int64)\n",
    "    batch['net_input']['src_tokens'] = batch['net_input']['src_tokens'].type(\n",
    "        torch.LongTensor)\n",
    "\n",
    "    batch = utils.move_to_cuda(batch) if use_cuda else batch\n",
    "    \n",
    "    # assume no prefix tokens to initialise decoded output\n",
    "    prefix_tokens = None\n",
    "    \n",
    "    gen_timer = StopwatchMeter()\n",
    "    gen_timer.start()\n",
    "    # target_tokens = None\n",
    "    hypos = task.inference_step(generator, model, batch, prefix_tokens)\n",
    "    num_generated_tokens = sum(len(h[0]['tokens']) for h in hypos)\n",
    "    gen_timer.stop(num_generated_tokens)\n",
    "    \n",
    "    device = 'GPU' if use_cuda else 'CPU'\n",
    "    \n",
    "    print('Generated {} sentences ({} tokens) in {:.1f}s on {}\\n'.format(\n",
    "        batch['nsentences'], gen_timer.n, gen_timer.sum, device))\n",
    "    \n",
    "    # Process top predictions\n",
    "    for j, hypo in enumerate(hypos[0][:args.nbest]):\n",
    "        hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
    "            hypo_tokens=hypo['tokens'].int().cpu(),\n",
    "            src_str=tokens,\n",
    "            alignment=hypo['alignment'],\n",
    "            align_dict=align_dict,\n",
    "            tgt_dict=task.target_dictionary,\n",
    "            remove_bpe=args.remove_bpe,\n",
    "            # extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n",
    "        )\n",
    "        detok_hypo_str = decode_fn(hypo_str)\n",
    "        \n",
    "        print(detok_hypo_str)\n",
    "        print()\n",
    "        \n",
    "        if target:\n",
    "            # Generate and compute BLEU score\n",
    "            if args.sacrebleu:\n",
    "                scorer = bleu.SacrebleuScorer()\n",
    "            else:\n",
    "                scorer = bleu.Scorer(\n",
    "                    task.target_dictionary.pad(),\n",
    "                    task.target_dictionary.eos(),\n",
    "                    task.target_dictionary.unk())\n",
    "            \n",
    "#             if align_dict is not None or args.remove_bpe is not None:\n",
    "                # Convert back to tokens for evaluation with unk replacement and/or without BPE\n",
    "            target_tokens = task.target_dictionary.encode_line(target, add_if_not_exist=True)\n",
    "            hypo_tokens = task.target_dictionary.encode_line(detok_hypo_str, add_if_not_exist=True)\n",
    "            \n",
    "            if hasattr(scorer, 'add_string'): # i.e. if using SacreBLEU\n",
    "                scorer.add_string(target, detok_hypo_str)\n",
    "            else:\n",
    "                scorer.add(target_tokens, hypo_tokens) # calculate n-gram overlap on vectorized texts \n",
    "            \n",
    "            print(scorer.result_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate review responses with loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (102 tokens) in 10.3s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . auch wenn wir es bedauern , dass sie keine besseren Erfahrungen bei uns gemacht haben , hilft uns ihre Beurteilung dazu zu lernen und uns zu verbessern . wir arbeiten daran , es unseren G√§sten an nichts fehlen zu lassen . wenn sie uns die Chance geben , ihr Vertrauen zur√ºck zu gewinnen , garantieren wir ihnen , unser Bestes zu geben , damit sie genauso gute Erfahrungen mit uns machen k√∂nnen , wie andere G√§ste . <SALUTATION>\n",
      "\n",
      "BLEU4 = 0.00, 43.6/12.0/1.0/0.0 (BP=0.837, ratio=0.849, syslen=101, reflen=119)\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "das Allerletzte ---SEP--- wir haben bereits vor Monaten \n",
    "zwei Doppelzimmer mit getrennten Betten gebucht , \n",
    "die Kreditkarte wurde lange vor der Ankunft belastet , \n",
    "obwohl die Zimmer bezahlt sind wird die Kreditkarte bei \n",
    "der Ankunft noch einmal mit <DIGIT> Euro Garantie belastet \n",
    "( mehr als der Preis der bereits bezahlten √úbernachtungen ) , \n",
    "bei der Ankunft im Hotel ist die Reception v√∂llig √ºberlaufen , \n",
    "Wartezeit eine halbe Stunde , die reservierten und bezahlten \n",
    "Zimmer stehen nicht zur Verf√ºgung , das Hotel sei ausgebucht , \n",
    "wir bekommen lediglich zwei Zimmer mit kleinen Doppelbetten , \n",
    "ein Zimmer nicht sehr sauber , es liegt im Eingangsbereich \n",
    "M√ºll auf dem Boden , das ganze ist eine Zumutung , \n",
    "Personal an der Rezeption genervt und unfreundlich , \n",
    "dieses Hotel besser meiden , \n",
    "P.S. : in der Hotelhalle ist auch noch Erbrochenes auf dem Fu√üboden\n",
    "\"\"\"\n",
    "target = \"\"\"\n",
    "<GREETING> W , vielen Dank f√ºr ihren Aufenthalt und f√ºr die Zeit ,\n",
    "die sie sich genommen haben um uns dieses Feedback zu schreiben .\n",
    "wir m√∂chten uns nat√ºrlich f√ºr den negativen Eindruck ,\n",
    "den sie hier offensichtlich bekommen haben entschuldigen .\n",
    "da uns die Zufriedenheit unserer G√§ste sehr am Herzen liegt ,\n",
    "m√∂chten wir der Sache gerne auf den Grund gehen und bitten sie daher ,\n",
    "uns pers√∂nlich per E-Mail zu kontaktieren ,\n",
    "damit wir in diesem Fall recherchieren k√∂nnen .\n",
    "dies dient zum einen zur internen Verbesserung ,\n",
    "zum anderen m√∂chten wir ihnen nat√ºrlich gerne zeigen ,\n",
    "dass wir es besser k√∂nnen und eine L√∂sung f√ºr diese Herausforderung finden .\n",
    "wir freuen uns auf ihre R√ºckmeldung .\n",
    "\"\"\"\n",
    "senti = 1\n",
    "cate = 1\n",
    "rate = 1\n",
    "\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (49 tokens) in 3.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir freuen uns , dass ihnen der Aufenthalt bei uns gefallen hat und hoffen , sie bald wieder bei uns begr√º√üen zu d√ºrfen ! <SALUTATION>\n",
      "\n",
      "BLEU4 = 0.00, 29.2/8.5/2.2/0.0 (BP=1.000, ratio=1.116, syslen=48, reflen=43)\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "das war das letzte Mal ---SEP--- unprofessionelle , \n",
    "aggressive und gewaltt√§tige T√ºr ohne jede Klasse und \n",
    "ohne die F√§higkeit gewaltfrei zu deeskalieren\"\"\"\n",
    "target = \"\"\"\n",
    "<GREETING> , wir bedauern sehr , dass sie so unangenehme\n",
    "eine Erfahrung bei uns machen Mussten . schreiben sie uns\n",
    "sehr gerne eine E-Mail an <EMAIL> , damit wir mehr Details\n",
    "√ºber diesen Abend erfahren und gemeinsam eine L√∂sung finden k√∂nnen . <SALUTATION>\"\"\"\n",
    "senti = 1\n",
    "cate = 2\n",
    "rate = 1\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (37 tokens) in 4.9s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir hoffen , sie bald wieder bei uns begr√º√üen zu d√ºrfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"leider sehr entt√§uschend ---SEP--- wir waren \n",
    "an einem sonnigen Donnerstagabend auf der Terrasse von The Butcher .\n",
    "auf unsere Speisekarte mussten wir erst einmal <DIGIT> Minuten warten ,\n",
    "die Bedienung erkl√§rte uns , dass Zurzeit keine Karten verf√ºgbar seien .\n",
    "die Bedienung war zwar freundliche , wirkte jedoch gestresst , \n",
    "weil alle Tische besetzt waren . \n",
    "wir bestellten zwei vegane Burger , da drei der vegetarischen Burger\n",
    "laut Karte Vegan zubereitet werden k√∂nne . die Bedienung erkl√§rte uns dann aber ,\n",
    "dass dies nur bei einem Burger m√∂glich sei . \n",
    "als die Burger nach einer halben Stunde serviert wurden , enthielten sie K√§se .\n",
    "also schickten wir sie zur√ºck in die K√ºche . \n",
    "nach einer Viertelstunde kamen die Burger zur√ºck - diesmal mit einem \" V \" gekennzeichnet .\n",
    "sie enthielten noch immer K√§se . erst nach insgesamt <DIGIT> Stunden Wartezeit\n",
    "erhielten wir schliesslich die bestellten veganen Burger , \n",
    "dabei handelte es sich aber eher um ein Br√∂tchen mir Gem√ºse ohne Sauce . ich habe Verst√§ndnis daf√ºr , dass nicht in jedem Restaurant Vegan gegessen werden kann . wer dies aber auf die Karte schreibt , sollte doch wenigstens wissen , was Vegan bedeutet . auch irritierend fand ich , dass ein Burger-Restaurant , das auf der Karte damit wirbt , besonders nachhaltig zu sein , alle Getr√§nke im Plastik-Becher serviert . nachhaltig geht anders .\"\"\"\n",
    "senti = 2 # 7\n",
    "cate = 2\n",
    "rate = 1\n",
    "\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (102 tokens) in 12.3s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . auch wenn wir es bedauern , dass sie keine besseren Erfahrungen bei uns gemacht haben , hilft uns ihre Beurteilung dazu zu lernen und uns zu verbessern . wir arbeiten daran , es unseren G√§sten an nichts fehlen zu lassen . wenn sie uns die Chance geben , ihr Vertrauen zur√ºck zu gewinnen , garantieren wir ihnen , unser Bestes zu geben , damit sie genauso gute Erfahrungen mit uns machen k√∂nnen , wie andere G√§ste . <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"falsche Zusagen ( schriftlich ) ---SEP--- unsere Anreise erfolgte gegen <DIGIT> Uhr mit <DIGIT> m√ºden Kindern nach einer <DIGIT> st√ºndigen Autofahrt . ich wurde noch begr√º√üt , mein Mann der <DIGIT> Min sp√§ter nach kam nicht ( kein Problem ) . vor der Buchung wurde uns schriftlich ein Doppelzimmer mit Verbindungst√ºr best√§tigt . vor Ort , wusste keiner etwas davon . Kinderzimmer auf einer Etage , Elternschlafzimmer auf einer anderen ! ? ? ? nach eindringlichem Bitten eine L√∂sung zu finden , wurde mir gesagt sie seien ausgebucht und das ist eben so ! ? ich habe wehement darum gebeten eine L√∂sung zu finden - beide angestellte <DIGIT> Min weg ! ? dann wurde mir gesagt , sie versuchen es- kann dauern ..... auf meine Frage , wie lange ( mittlerweile <DIGIT> Uhr ) war die Antwort - keine Ahnung ü§î . unfassbar wie Unprofessionell . nach eindringlichem Bitten mir eine ungef√§hre Zeit zu nennen , <DIGIT> Min , <DIGIT> Min oder eine Stunde wurde mir gesagt eher 1stunde . ich habe dankend abgelehnt und darum gebeten den Vorgang zu stornieren . ich h√§tte dieses Hotel niemals ohne Zusage der Verbindungst√ºr gew√§hlt . vor Ort waren die Mitarbeiter echt frech und ignorant - ich bin schwer entt√§uscht . das einzig positive , das sie den Vorgang ohne Kosten storniert haben\"\"\"\n",
    "senti = 1\n",
    "cate = 1\n",
    "rate = 1\n",
    "\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (49 tokens) in 4.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir freuen uns , dass ihnen der Aufenthalt bei uns gefallen hat und hoffen , sie bald wieder bei uns begr√º√üen zu d√ºrfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"Miserabel ---SEP--- im Hotel Seiler ist das Wort Kulanz nicht existent . wir h√§tten wenigstens ein Entgegenkommen in Form.Z . b.eines Gutscheines erwartet . das Hotel Seiler haben wir aus unserer Liste gestrichen . wir werden auch entsprechend Werbung f√ºr das Haus machen .\"\"\"\n",
    "senti = 4\n",
    "cate = 1\n",
    "rate = 1\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (49 tokens) in 4.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir freuen uns , dass ihnen der Aufenthalt bei uns gefallen hat und hoffen , sie bald wieder bei uns begr√º√üen zu d√ºrfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"stinkend statt dufte ---SEP--- das gesamte Hotel eoch nach Toilette , da es ein paar Wochen zuvor ein Abwasser-Rohrbruch gab . das Badwar ungen√ºgend geputzt und die Klimaanlage lie√ü sich nicht regeln . das Zimmer war sehr warm . das Personal am Check-In lehnte eine Reklamation ab und wies mehrere G√§ste in arroganter und unfreundlicher Weise ab .\"\"\"\n",
    "senti = 3 #7\n",
    "cate = 1\n",
    "rate = 1\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (101 tokens) in 9.4s on CPU\n",
      "\n",
      "<GREETING> vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . auch wenn wir es bedauern , dass sie keine besseren Erfahrungen bei uns gemacht haben , hilft uns ihre Beurteilung dazu zu lernen und uns zu verbessern . wir arbeiten daran , es unseren G√§sten an nichts fehlen zu lassen . wenn sie uns die Chance geben , ihr Vertrauen zur√ºck zu gewinnen , garantieren wir ihnen , unser Bestes zu geben , damit sie genauso gute Erfahrungen mit uns machen k√∂nnen , wie andere G√§ste . <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"entt√§uschend ---SEP--- der Wein ( Pinot Grigio und Frascati ) war absolut nicht zu empfehlen , billige No-Name Ware , das Essen geschmacklos ( Pizza Frutti Di Mare mit Meeresfr√ºchte-Imitat aus Surimi ) . die Damen im Service freundlich jedoch der Patrone beim Bezahlen der Rechnung unfreundlich und arrogant . nie wieder !\"\"\"\n",
    "senti = 3\n",
    "cate = 2\n",
    "rate = 1\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (49 tokens) in 4.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir freuen uns , dass ihnen der Aufenthalt bei uns gefallen hat und hoffen , sie bald wieder bei uns begr√º√üen zu d√ºrfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"es war besser als befriedigend aber nicht sehr gut ---SEP--- Gesamteindruck super , Preise f√ºr Garage und Fr√ºhst√ºck sind generell in allen Hotels zu hoch . Preis / Nacht w√§re besser zu akzeptieren wenn es Fitness/Wellness O.√Ñ. geben w√ºrde . dazu k√∂nnten Z.B. Info ¬¥ s zu Partnern ausliegen .\"\"\"\n",
    "senti = 6\n",
    "cate = 1\n",
    "rate = 3\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (37 tokens) in 4.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir hoffen , sie bald wieder bei uns begr√º√üen zu d√ºrfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"Weihnachten im Parkhotel lindner Oberstaufen ---SEP--- unvergessliche Weihnachtsatmosph√§re im Parkhotel Lindner , zusammen feiern in einer grossen Familie , sehr gediegen mit Harfenmusik , Weihnachtsgeschichten h√∂ren und zusammen Weihnachtslieder singen unter einem wunderbar geschm√ºckten Baum , das war Weihnachten <DIGIT> . das freundliche aufgestellte Personal verw√∂hnte uns G√§ste mit auserlesenen Speisen aus der K√ºche von Herrn Wagenblast . es war einmal mehr ein unvergesslicher Aufenthalt .\"\"\"\n",
    "senti = 8\n",
    "cate = 1\n",
    "rate = 5\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
