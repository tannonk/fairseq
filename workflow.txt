experimetns2:
giving encoder2 to the model:
2020-12-11 15:21:33 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2
2020-12-11 15:21:33 | INFO | fairseq_cli.generate | Translated 48 sentences (1897 tokens) in 3.8s (12.49 sentences/s, 493.47 tokens/s)
2020-12-11 15:21:33 | INFO | fairseq_cli.generate | Generate test with beam=10: BLEU4 = 9.39, 50.9/21.9/12.7/8.2 (BP=0.509, ratio=0.597, syslen=1849, reflen=3098)

experiments:
2020-12-14 14:35:45 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base2
2020-12-14 14:35:45 | INFO | fairseq_cli.generate | Translated 48 sentences (1874 tokens) in 4.3s (11.22 sentences/s, 437.91 tokens/s)
2020-12-14 14:35:45 | INFO | fairseq_cli.generate | Generate test with beam=10: BLEU4 = 9.47, 53.1/22.9/13.0/8.3 (BP=0.498, ratio=0.589, syslen=1826, reflen=3098)



Discovered this today
# --model-parallel-size: total number of GPUs to parallelize model over
# ModuleNotFoundError: No module named 'fairseq.model_parallel.megatron.mpu'

----------------------

Should I work with a Monolingual Dictionary and merge all three inputs?

model: simple_lstm
task: translation
data: rr_raw_toyset

I needed to specify "--dataset-impl raw" when training a model on raw preprocessed data.

no_progress_bar ::: False
log_interval ::: 100
log_format ::: None
tensorboard_logdir :::
seed ::: 1
cpu ::: False
tpu ::: False
bf16 ::: False
fp16 ::: False
memory_efficient_bf16 ::: False
memory_efficient_fp16 ::: False
fp16_no_flatten_grads ::: False
fp16_init_scale ::: 128
fp16_scale_window ::: None
fp16_scale_tolerance ::: 0.0
min_loss_scale ::: 0.0001
threshold_loss_scale ::: None
user_dir ::: None
empty_cache_freq ::: 0
all_gather_list_size ::: 16384
model_parallel_size ::: 1
checkpoint_suffix :::
quantization_config_path ::: None
profile ::: False
criterion ::: cross_entropy
tokenizer ::: None
bpe ::: None
optimizer ::: adam
lr_scheduler ::: fixed
task ::: translation
num_workers ::: 1
skip_invalid_size_inputs_valid_test ::: True
max_tokens ::: 10000
max_sentences ::: None
required_batch_size_multiple ::: 8
dataset_impl ::: None
data_buffer_size ::: 10
train_subset ::: train
valid_subset ::: valid
validate_interval ::: 1
fixed_validation_seed ::: None
disable_validation ::: False
max_tokens_valid ::: 10000
max_sentences_valid ::: None
curriculum ::: 0
distributed_world_size ::: 1
distributed_rank ::: 0
distributed_backend ::: nccl
distributed_init_method ::: None
distributed_port ::: -1
device_id ::: 0
distributed_no_spawn ::: False
ddp_backend ::: c10d
bucket_cap_mb ::: 25
fix_batches_to_gpus ::: False
find_unused_parameters ::: False
fast_stat_sync ::: False
broadcast_buffers ::: False
distributed_wrapper ::: DDP
slowmo_momentum ::: None
slowmo_algorithm ::: LocalSGD
localsgd_frequency ::: 3
nprocs_per_node ::: 1
arch ::: tutorial_simple_lstm
max_epoch ::: 2
max_update ::: 0
stop_time_hours ::: 0
clip_norm ::: 0.0
sentence_avg ::: False
update_freq ::: [1]
lr ::: [0.005]
min_lr ::: -1
use_bmuf ::: False
save_dir ::: /home/user/shaita/data/fairseq_test/checkpoints/rrt_02_10000
restore_file ::: checkpoint_last.pt
reset_dataloader ::: False
reset_lr_scheduler ::: False
reset_meters ::: False
reset_optimizer ::: False
optimizer_overrides ::: {}
save_interval ::: 1
save_interval_updates ::: 0
keep_interval_updates ::: -1
keep_last_epochs ::: -1
keep_best_checkpoints ::: -1
no_save ::: False
no_epoch_checkpoints ::: False
no_last_checkpoints ::: False
no_save_optimizer_state ::: False
best_checkpoint_metric ::: loss
maximize_best_checkpoint_metric ::: False
patience ::: -1
encoder_dropout ::: 0.2
decoder_dropout ::: 0.2
adam_betas ::: (0.9, 0.999)
adam_eps ::: 1e-08
weight_decay ::: 0.0
use_old_adam ::: False
force_anneal ::: None
lr_shrink ::: 0.5
warmup_updates ::: 0
data ::: /home/user/shaita/data/fairseq_test/rr_raw_toyset/
source_lang ::: review
target_lang ::: response
load_alignments ::: False
left_pad_source ::: True
left_pad_target ::: False
max_source_positions ::: 400
max_target_positions ::: 400
upsample_primary ::: 1
truncate_source ::: False
num_batch_buckets ::: 0
eval_bleu ::: False
eval_bleu_detok ::: space
eval_bleu_detok_args ::: None
eval_tokenized_bleu ::: False
eval_bleu_remove_bpe ::: None
eval_bleu_args ::: None
eval_bleu_print_samples ::: False
no_seed_provided ::: True
encoder_embed_dim ::: 256
encoder_hidden_dim ::: 256
decoder_embed_dim ::: 256
decoder_hidden_dim ::: 256

***

When I try to train a model on my raw data I get the following error:

Traceback (most recent call last):
  File "/home/user/shaita/anaconda3/envs/fairseq/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/user/shaita/fairseq/fairseq_cli/train.py", line 360, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/user/shaita/fairseq/fairseq/distributed_utils.py", line 198, in call_main
    main(args, **kwargs)
  File "/home/user/shaita/fairseq/fairseq_cli/train.py", line 63, in main
    task.load_dataset(valid_sub_split, combine=False, epoch=1)
  File "/home/user/shaita/fairseq/fairseq/tasks/translation.py", line 274, in load_dataset
    shuffle=(split != 'test'),
  File "/home/user/shaita/fairseq/fairseq/tasks/translation.py", line 73, in load_langpair_dataset
    raise FileNotFoundError('Dataset not found: {} ({})'.format(split, data_path))
FileNotFoundError: Dataset not found: valid (/home/user/shaita/data/fairseq_test/rr_raw_toyset/)

(fairseq) shaita@rattle:~/fairseq$ ls /home/user/shaita/data/fairseq_test/rr_raw_toyset/
dict.response.txt  preprocess.log                 test.review-response.review     train.review-response.review    valid.review-response.review
dict.review.txt    test.review-response.response  train.review-response.response  valid.review-response.response
