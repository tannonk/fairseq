# ####################### PREPROCESSING ######################################

fairseq-preprocess \
    --source-lang description \
    --target-lang response \
    --trainpref  /home/user/shaita/data/fairseq_test/TAGK_toyset/train \
    --validpref /home/user/shaita/data/fairseq_test/TAGK_toyset/valid \
    --testpref  /home/user/shaita/data/fairseq_test/TAGK_toyset/test \
    --dataset-impl raw \
    --joined-dictionary \
    --destdir /home/user/shaita/data/fairseq_test/dresp_raw_toyset \
    --task translation
    rrgen_translation_knowldg


# ######################### TRAINING ########################################

available data binary:
    - /home/user/shaita/data/fairseq_test/rr_bin_full
    - /home/user/shaita/data/fairseq_test/rr_bin_toyset

available data raw no description:
    - /home/user/shaita/data/fairseq_test/rr_raw_full
    - /home/user/shaita/data/fairseq_test/rr_raw_toyset

available data raw with description:
    - /home/user/shaita/data/fairseq_test/rrd_raw_full
    - /home/user/shaita/data/fairseq_test/rrd_raw_toyset

### simple_lstm

    PYTHONBREAKPOINT=0 \
    CUDA_LAUNCH_BLOCKING=1

    PYTHONBREAKPOINT=0 \
    CUDA_VISIBLE_DEVICES=2 fairseq-train /home/user/shaita/data/fairseq_test/experiment/ \
    --arch tutorial_simple_lstm \
    --skip-invalid-size-inputs-valid-test \
    --save-dir /home/user/shaita/data/fairseq_test/checkpoints/experiment_1 \
    -s review \
    -t response \
    --knowledge description \
    --optimizer adam \
    --lr 0.005 \
    --lr-shrink 0.5 \
    --max-tokens 10000 \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --dataset-impl raw \
    --task=translation \
    --max-epoch 1

    --num-workers 0 \
    --ddp-backend=no_c10d \
    --task rrgen_translation_knowldg
    -s2 description \

    Produces exactly the same output for all samples: 	<GREETING> thank you for taking the time to review your recent stay with us . i am delighted to hear that you enjoyed your stay with us . we look forward to welcoming you back again soon . <SALUTATION>


## lstm_2encoders

    #### review == source1, knowledge == source2 ####

    PYTHONBREAKPOINT=0 \
    CUDA_LAUNCH_BLOCKING=1

    PYTHONBREAKPOINT=0 \
    CUDA_VISIBLE_DEVICES=2 \
    fairseq-train /home/user/shaita/data/fairseq_test/experiment/ \
    --arch lstm_2encoders_arch \
    --skip-invalid-size-inputs-valid-test \
    --save-dir /home/user/shaita/data/fairseq_test/checkpoints/lstm_2encoders_0 \
    -s review \
    -t response \
    --knowledge description \
    --optimizer adam \
    --lr 0.005 \
    --lr-shrink 0.5 \
    --max-tokens 10000 \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --dataset-impl raw \
    --task=translation \
    --num-workers 0 \
    --max-epoch 1

    --cpu \
    --ddp-backend=no_c10d \
    --task rrgen_translation_knowldg
    -s2 description \


    #### review == source1, knowledge == source2 ####

    PYTHONBREAKPOINT=0 \
    CUDA_VISIBLE_DEVICES=2 \
    fairseq-train /home/user/shaita/data/fairseq_test/experiment/ \
    --arch my_lstm_2encoders \
    --skip-invalid-size-inputs-valid-test \
    --save-dir /home/user/shaita/data/fairseq_test/checkpoints/lstm_2encoders_switched \
    -s description \
    -t response \
    --knowledge review \
    --optimizer adam \
    --lr 0.005 \
    --lr-shrink 0.5 \
    --max-tokens 10000 \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --dataset-impl raw \
    --task=translation \
    --max-epoch 10

+ sending knowledge thru src_tokens worked, this means it's not about the descriptions themselves

### rrgen_lstm

train_src_rg: ~/scratch/fairseq_materialss/rrgen/en/data_bin_rg
    fairseq-train /home/user/shaita/data/fairseq_test/data_bin_rg/ \
    --arch rrgen_lstm_arch \ ?
    --task rrgen_translation_knowldg \
    --dataset-impl mmap \
    --max-epoch 10 \
    --max-tokens 4000 \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --lr 0.001 \
    ## --encoder-embed-path ~/scratch/embeddings/FT_en.w2v.txt \
    --share-all-embeddings \
    --encoder-embed-dim 100 \
    --decoder-embed-dim 100 \
    --decoder-out-embed-dim 100 \
    --encoder-hidden-size 200 \
    --decoder-hidden-size 200 \
    --save-dir /home/user/shaita/data/fairseq_test/checkpoints/??? \
    --use-knowledge \
    --skip-invalid-size-inputs-valid-test


# ######################### DECODING #####################################

you can generate translations using fairseq-generate (for binarized data) or fairseq-interactive (for raw text).

    PYTHONBREAKPOINT=0 \
    CUDA_VISIBLE_DEVICES=0 fairseq-generate \
    /home/user/shaita/data/fairseq_test/experiment/ \
    --path /home/user/shaita/data/fairseq_test/checkpoints/experiment_2a/checkpoint_best.pt \
    --skip-invalid-size-inputs-valid-test \
    -s review \
    -t response \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --beam 10 \
    --dataset-impl raw \
    --knowledge description \
    --batch-size 16 \
    --data-buffer-size 4 \
    --num-workers 4

    --sampling \
    --nbest 10\
    --sampling-topk 30\

<GREETING> thank you for taking the time to write a review on your recent stay with us . we are so pleased to hear that you enjoyed your stay with us . we look forward to welcoming you back again soon . <SALUTATION>

1   thank you for taking the time to write a review on your recent stay with us
13  thank you for taking the time to write a review
112 thank you for taking the time
37  the time to write
17  write a review
3   review on your recent stay
4   on your recent stay with us
55  your recent stay
60 recent stay
128 stay with
216 with us
125 stay with us

1   we are so pleased to hear that you enjoyed your stay with us
3   we are so pleased to hear
11  that you enjoyed your stay with us

3   we look forward to welcoming you back again soon
109 we look forward
5   welcoming you back again soon
99  welcoming you
85  to welcoming you
16  back again soon
37  back again
56  again soon


ADDED ATTENTION: more variation
<GREETING> thank you for taking the time to write a review of your recent stay with us . we look forward to welcoming you back again soon . <SALUTATION>

<GREETING> thank you for taking the time to write a review . we look forward to welcoming you back in the future . <SALUTATION>


# ######################## MISC ##########################################


CUDA_VISIBLE_DEVICES=2 fairseq-train data-bin/iwslt14.tokenized.de-en   --arch tutorial_simple_lstm   --encoder-dropout 0.2 --decoder-dropout 0.2   --optimizer adam --lr 0.005 --lr-shrink 0.5   --max-tokens 12000

    cp zhao_cloned/dataset/ground_large_pp/test_source.txt data/fairseq_test/test.review
    cp zhao_cloned/dataset/ground_large_pp/test_target.txt data/fairseq_test/test.response
    cp zhao_cloned/dataset/ground_large_pp/train_target.txt data/fairseq_test/train.response
    cp zhao_cloned/dataset/ground_large_pp/train_source.txt data/fairseq_test/train.review
    cp zhao_cloned/dataset/ground_large_pp/valid_source.txt data/fairseq_test/valid.review
    cp zhao_cloned/dataset/ground_large_pp/valid_target.txt data/fairseq_test/valid.response
    cp zhao_cloned/dataset/ground_large_pp/valid_source_2.txt data/fairseq_test/valid.description
    cp zhao_cloned/dataset/ground_large_pp/test_source_2.txt data/fairseq_test/test.description
    cp zhao_cloned/dataset/ground_large_pp/train_source_2.txt data/fairseq_test/train.description


/home/user/shaita/data/fairseq_test/TAGK_dataset  TAGK_toyset


# ######################## TANNON'S ##########################################

    categories.json
    test.rating    test.response_rg     test.sentiment
    train.rating   train.response_rg    train.sentiment
    valid.rating   valid.response_rg    valid.sentiment

    test.domain    test.response        test.review
    train.domain   train.response       train.review
    valid.domain   valid.response       valid.review


fairseq-preprocess \
        --source-lang review \
        --target-lang response_rg \
        --trainpref /srv/scratch2/kew/fairseq_materials/rrgen/en/data_raw/train \
        --validpref /srv/scratch2/kew/fairseq_materials/rrgen/en/data_raw/valid \
        --testpref /srv/scratch2/kew/fairseq_materials/rrgen/en/data_raw/test \
        --dataset-impl raw \
        --task rrgen_translation \
        --joined-dictionary \
        --destdir /srv/scratch2/kew/fairseq_materials/rrgen/en/data_bin_rg

--------------------------------------------------------------------------------

# NB. executed on s3it volta
train_src_rg: ~/scratch/fairseq_materialss/rrgen/en/data_bin_rg
    fairseq-train ~/scratch/fairseq_materials/rrgen/en/data_bin_rg/ \
    --arch rrgen_lstm_arch \
    --task rrgen_translation \
    --dataset-impl mmap \
    --max-epoch 10 \
    --max-tokens 4000 \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --lr 0.001 \
    --encoder-embed-path ~/scratch/embeddings/FT_en.w2v.txt \
    --share-all-embeddings \
    --encoder-embed-dim 100 \
    --decoder-embed-dim 100 \
    --decoder-out-embed-dim 100 \
    --encoder-hidden-size 200 \
    --decoder-hidden-size 200 \
    --save-dir ~/scratch/fairseq_materials/rrgen/en/ft100src_rg \
    --use-sentiment sentiment \
    --use-category domain \
    --use-rating rating \
    --skip-invalid-size-inputs-valid-test

--------------------------------------------------------------------------------

decode_src_rg_greedy:
    CUDA_VISIBLE_DEVICES=6 nohup fairseq-generate \
    /srv/scratch2/kew/fairseq_materials/rrgen/en/data_bin_rg \
    --path /srv/scratch2/kew/fairseq_materials/rrgen/en/ft100src_rg/checkpoints/checkpoint_best.pt \
    -s review \
    -t response_rg \
    --task rrgen_translation \
    --dataset-impl mmap \
    --batch-size 16 \
    --data-buffer-size 4 \
    --num-workers 4 \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --skip-invalid-size-inputs-valid-test \
    --nbest 5 \
    --beam 5 \
    --model-overrides "{'encoder_embed_path': '/srv/scratch2/kew/fasttext/FT_en.w2v.txt', 'decoder_embed_path': '/srv/scratch2/kew/fasttext/FT_en.w2v.txt'}" \
    --use-sentiment sentiment \
    --use-category domain \
    --use-rating rating > /srv/scratch2/kew/fairseq_materials/rrgen/en/ft100src_rg/nbest5.txt &


# ######################## fairseq's ##########################################
    fairseq-preprocess \
        --trainpref names/train \
        --validpref names/valid \
        --testpref names/test \
        --source-lang input \
        --target-lang label \
        --destdir names-bin \
        --dataset-impl raw


    cd examples/translation/
    bash prepare-iwslt14.sh
    cd ../..
    TEXT=examples/translation/iwslt14.tokenized.de-en
    # code  test.de  test.en  tmp/ train.de  train.en  valid.de  valid.en
    fairseq-preprocess \
        --source-lang de \
        --target-lang en \
        --trainpref $TEXT/train \
        --validpref $TEXT/valid \
        --testpref $TEXT/test \
        --destdir data-bin/iwslt14.tokenized.de-en

--------------------------------------------------------------------------------

fairseq-generate data-bin/iwslt14.tokenized.de-en \
  --path checkpoints/checkpoint_best.pt \
  --beam 5 \
