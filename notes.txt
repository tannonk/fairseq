
# ####################### PREPROCESSING ######################################

        fairseq-preprocess \
            --trainpref names/train \
            --validpref names/valid \
            --testpref names/test \
            --source-lang input \
            --target-lang label \
            --destdir names-bin \
            --dataset-impl raw


        cd examples/translation/
        bash prepare-iwslt14.sh
        cd ../..
        TEXT=examples/translation/iwslt14.tokenized.de-en
        # code  test.de  test.en  tmp/ train.de  train.en  valid.de  valid.en
        fairseq-preprocess \
            --source-lang de \
            --target-lang en \
            --trainpref $TEXT/train \
            --validpref $TEXT/valid \
            --testpref $TEXT/test \
            --destdir data-bin/iwslt14.tokenized.de-en

--------------------------------------------------------------------------------

TANNON'S:
    categories.json
    test.rating    test.response_rg     test.sentiment
    train.rating   train.response_rg    train.sentiment
    valid.rating   valid.response_rg    valid.sentiment

    test.domain    test.response        test.review
    train.domain   train.response       train.review
    valid.domain   valid.response       valid.review


fairseq-preprocess \
        --source-lang review \
        --target-lang response_rg \
        --trainpref /srv/scratch2/kew/fairseq_materials/rrgen/en/data_raw/train \
        --validpref /srv/scratch2/kew/fairseq_materials/rrgen/en/data_raw/valid \
        --testpref /srv/scratch2/kew/fairseq_materials/rrgen/en/data_raw/test \
        --dataset-impl mmap \
        --task rrgen_translation \
        --joined-dictionary \
        --destdir /srv/scratch2/kew/fairseq_materials/rrgen/en/data_bin_rg

--------------------------------------------------------------------------------

MINE:

        fairseq-preprocess \
            --source-lang review \
            --target-lang response \
            --trainpref  /home/user/shaita/data/fairseq_test/train \
            --validpref /home/user/shaita/data/fairseq_test/valid \
            --testpref  /home/user/shaita/data/fairseq_test/test \
            --dataset-impl raw \
            --task rrgen_translation_knowldg \
            --joined-dictionary \
            --destdir /home/user/shaita/data/fairseq_test/data_raw_rg

Wrote preprocessed data to /home/user/shaita/data/fairseq_test/data_bin_rg


# ######################### TRAINING ########################################

TANNON'S:

# NB. executed on s3it volta
train_src_rg: ~/scratch/fairseq_materialss/rrgen/en/data_bin_rg
    fairseq-train ~/scratch/fairseq_materials/rrgen/en/data_bin_rg/ \
    --arch rrgen_lstm_arch \
    --task rrgen_translation \
    --dataset-impl mmap \
    --max-epoch 10 \
    --max-tokens 4000 \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --lr 0.001 \
    --encoder-embed-path ~/scratch/embeddings/FT_en.w2v.txt \
    --share-all-embeddings \
    --encoder-embed-dim 100 \
    --decoder-embed-dim 100 \
    --decoder-out-embed-dim 100 \
    --encoder-hidden-size 200 \
    --decoder-hidden-size 200 \
    --save-dir ~/scratch/fairseq_materials/rrgen/en/ft100src_rg \
    --use-sentiment sentiment \
    --use-category domain \
    --use-rating rating \
    --skip-invalid-size-inputs-valid-test

--------------------------------------------------------------------------------

MINE:

### simple_lstm

    CUDA_VISIBLE_DEVICES=1 fairseq-train /home/user/shaita/data/fairseq_test/data_bin_rg/ \
    --arch tutorial_simple_lstm \
    --encoder-dropout 0.2 \
    --decoder-dropout 0.2 \
    --skip-invalid-size-inputs-valid-test \
    --save-dir /home/user/shaita/data/fairseq_test/model01_4000 \
      --optimizer adam \
      --lr 0.005 \
      --lr-shrink 0.5 \
      --max-tokens 4000 \
      --max-source-positions 400 \
      --max-target-positions 400 \
      --max-epoch 10


### rrgen_lstm

train_src_rg: ~/scratch/fairseq_materialss/rrgen/en/data_bin_rg
    fairseq-train /home/user/shaita/data/fairseq_test/data_bin_rg/ \
    --arch rrgen_lstm_arch \ ?
    --task rrgen_translation_knowldg \
    --dataset-impl mmap \
    --max-epoch 10 \
    --max-tokens 4000 \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --lr 0.001 \
    ## --encoder-embed-path ~/scratch/embeddings/FT_en.w2v.txt \
    --share-all-embeddings \
    --encoder-embed-dim 100 \
    --decoder-embed-dim 100 \
    --decoder-out-embed-dim 100 \
    --encoder-hidden-size 200 \
    --decoder-hidden-size 200 \
    --save-dir /home/user/shaita/data/fairseq_test/output \
    --use-knowledge \
    --skip-invalid-size-inputs-valid-test


# ######################### DECODING #####################################

fairseq-generate data-bin/iwslt14.tokenized.de-en \
  --path checkpoints/checkpoint_best.pt \
  --beam 5 \
  --remove-bpe

--------------------------------------------------------------------------------

TANNON'S:

decode_src_rg_greedy:
    CUDA_VISIBLE_DEVICES=6 nohup fairseq-generate \
    /srv/scratch2/kew/fairseq_materials/rrgen/en/data_bin_rg \
    --path /srv/scratch2/kew/fairseq_materials/rrgen/en/ft100src_rg/checkpoints/checkpoint_best.pt \
    -s review \
    -t response_rg \
    --task rrgen_translation \
    --dataset-impl mmap \
    --batch-size 16 \
    --data-buffer-size 4 \
    --num-workers 4 \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --skip-invalid-size-inputs-valid-test \
    --nbest 5 \
    --beam 5 \
    --model-overrides "{'encoder_embed_path': '/srv/scratch2/kew/fasttext/FT_en.w2v.txt', 'decoder_embed_path': '/srv/scratch2/kew/fasttext/FT_en.w2v.txt'}" \
    --use-sentiment sentiment \
    --use-category domain \
    --use-rating rating > /srv/scratch2/kew/fairseq_materials/rrgen/en/ft100src_rg/nbest5.txt &

--------------------------------------------------------------------------------

MINE:

    CUDA_VISIBLE_DEVICES=1 fairseq-generate \
    /home/user/shaita/data/fairseq_test/data_bin_rg/ \
    --path /home/user/shaita/data/fairseq_test/model01_4000/checkpoint_best.pt \
    --skip-invalid-size-inputs-valid-test \
    -s review \
    -t response \
    --max-source-positions 400 \
    --max-target-positions 400 \
    --beam 5



# ######################## MISC ##########################################


CUDA_VISIBLE_DEVICES=2 fairseq-train data-bin/iwslt14.tokenized.de-en   --arch tutorial_simple_lstm   --encoder-dropout 0.2 --decoder-dropout 0.2   --optimizer adam --lr 0.005 --lr-shrink 0.5   --max-tokens 12000

    cp zhao_cloned/dataset/ground_large_pp/test_source.txt data/fairseq_test/test.review
    cp zhao_cloned/dataset/ground_large_pp/test_target.txt data/fairseq_test/test.response
    cp zhao_cloned/dataset/ground_large_pp/train_target.txt data/fairseq_test/train.response
    cp zhao_cloned/dataset/ground_large_pp/train_source.txt data/fairseq_test/train.review
    cp zhao_cloned/dataset/ground_large_pp/valid_source.txt data/fairseq_test/valid.review
    cp zhao_cloned/dataset/ground_large_pp/valid_target.txt data/fairseq_test/valid.response
    cp zhao_cloned/dataset/ground_large_pp/valid_source_2.txt data/fairseq_test/valid.description
    cp zhao_cloned/dataset/ground_large_pp/test_source_2.txt data/fairseq_test/test.description
    cp zhao_cloned/dataset/ground_large_pp/train_source_2.txt data/fairseq_test/train.description
